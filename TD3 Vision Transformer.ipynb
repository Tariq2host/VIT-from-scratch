{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TD3: Vision Transformer (ViT)\n",
        "\n",
        "In this TD, you must modify this notebook to complete the code (**# TO DO comments**) and complete the **proposed experiments**. To do this,\n",
        "\n",
        "1. Fork this repository\n",
        "2. Clone your forked repository on your local computer\n",
        "3. Add your code and answer the questions\n",
        "4. Commit and push regularly\n",
        "\n",
        "**The last commit is due on Sunday, 14th January 2024**. Later commits will not be taken into account.\n",
        "\n",
        "As the computation is heavy, particularly during training, we encourage you to use a GPU. If your laptob is not equiped, you may use one of these remote jupyter servers, where you can select the execution on GPU :\n",
        "\n",
        "1) [jupyter.mi90.ec-lyon.fr](https://jupyter.mi90.ec-lyon.fr/)\n",
        "\n",
        "This server is accessible within the campus network. If outside, you need to use a VPN. Before executing the notebook, select the kernel \"Python PyTorch\" to run it on GPU and have access to PyTorch module.\n",
        "\n",
        "2) [Google Colaboratory](https://colab.research.google.com/)\n",
        "\n",
        "Before executing the notebook, select the execution on GPU : \"Exécution\" Menu -> \"Modifier le type d'exécution\" and select \"T4 GPU\". "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Goal of the TD\n",
        "\n",
        "Transformers have been introduced by [Vaswani et al. in 2017](https://arxiv.org/abs/1706.03762) in the context of NLP (Natural Language Processing), and particulary for Machine Translation.\n",
        "\n",
        "Its great success has led to its adaptation to various applications, including image classification. In this trend, [Dosovitskiy et al. in 2020](https://arxiv.org/abs/2010.11929) have proposed Vision Transformers (ViT) that we will study and implement from scratch in this TD.\n",
        "\n",
        "The principle is illustrated in the following picture from this paper.\n",
        "\n",
        "![Vision Tranformers](./figures/vit.png \"Vision Transformers\")\n",
        "\n",
        "First, an input image is “cut” into sub-images equally sized.\n",
        "\n",
        "Each such sub-image goes through a linear embedding. From then, each sub-image becomes a one-dimensional vector.\n",
        "\n",
        "A positional embedding is then added to these vectors (tokens). The positional embedding allows the network to know where each sub-image is positioned originally in the image. Without this information, the network would not be able to know where each such image would be placed, leading to potentially wrong predictions.\n",
        "\n",
        "These tokens are then passed, together with a special classification token, to the transformer encoders blocks, were each is composed of : A Layer Normalization (LN), followed by a Multi-head Self Attention (MSA) and a residual connection. Then a second LN, a Multi-Layer Perceptron (MLP), and again a residual connection. These blocks are connected back-to-back.\n",
        "\n",
        "Finally, a classification MLP head is used for the final classification only on the special classification token, which by the end of this process has global information about the picture.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementation of the ViT model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we import the required modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEmbaOA4Okuo",
        "outputId": "2bf953f2-2a18-44f3-c537-db8c6d58d4ee"
      },
      "outputs": [],
      "source": [
        "# Import modules\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets.mnist import MNIST\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this first experiment, we will use the MNIST dataset that contains 28x28 binary pixels images of hand-written digits ([0–9])."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "transform = ToTensor()\n",
        "\n",
        "train_set = MNIST(\n",
        "    root=\"datasets\", train=True, download=True, transform=transform\n",
        ")\n",
        "test_set = MNIST(\n",
        "    root=\"datasets\", train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_set, shuffle=True, batch_size=128)\n",
        "test_loader = DataLoader(test_set, shuffle=False, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \"Patchification\"\n",
        "The transformer encoder was originally developed with sequence data in mind, such as English sentences. However, as an image is not a sequence, we need to “sequencify” an image. To do this, we break it into multiple sub-images and map each sub-image to a vector.\n",
        "\n",
        "We do so by simply reshaping our input, which has size (N, C, H, W), where N is the batch size, C the number of channels and (H,W) the image dimension. In the case of MNIST, dimensions are (N, 1, 28, 28). The target dimension is (N, #Patches, Patch dimensionality), where the dimensionality of a patch is adjusted accordingly.\n",
        "\n",
        "In this example, we break each (1, 28, 28) into 7x7 patches (hence, each of size 4x4). That is, we are going to obtain 7x7=49 sub-images out of a single image.\n",
        "\n",
        "Thus, we reshape input (N, 1, 28, 28) to (N, PxP, HxC/P x WxC/P) = (N, 49, 16)\n",
        "\n",
        "Notice that, while each patch is a picture of size 1x4x4, we flatten it to a 16-dimensional vector. Also, in this case, we only had a single color channel. If we had multiple color channels, those would also have been flattened into the vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fxhHKKDFOoHp"
      },
      "outputs": [],
      "source": [
        "def patchify(images, n_patches):\n",
        "    n, c, h, w = images.shape\n",
        "\n",
        "    assert h == w, \"Patchify method is implemented for square images only\"\n",
        "\n",
        "    patches = torch.zeros(n, n_patches**2, h * w * c // n_patches**2)\n",
        "    patch_size = h // n_patches\n",
        "\n",
        "    for idx, image in enumerate(images):\n",
        "        for i in range(n_patches):\n",
        "            for j in range(n_patches):\n",
        "                patch = image[\n",
        "                    :,\n",
        "                    i * patch_size : (i + 1) * patch_size,\n",
        "                    j * patch_size : (j + 1) * patch_size,\n",
        "                ]\n",
        "                patches[idx, i * n_patches + j] = patch.flatten()\n",
        "    return patches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Linear embedding\n",
        "\n",
        "Now that we have our flattened patches, we can map each of them through a Linear mapping. While each patch was a 4x4=16 dimensional vector, the linear mapping can map to any arbitrary vector size. Thus, we will use for this a parameter `hidden_d` for \"hidden dimension\".\n",
        "\n",
        "In this example, we will use a hidden dimension of 8, but in principle, any number can be put here. We will thus be mapping each 16-dimensional patch to an 8-dimensional patch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Positional encoding\n",
        "\n",
        "Positional encoding allows the model to understand where each patch would be placed in the original image. While it is theoretically possible to learn such positional embeddings, previous work by [Vaswani et al. in 2017](https://arxiv.org/abs/1706.03762) suggests that we can just add sines and cosines waves.\n",
        "\n",
        "In particular, positional encoding adds high-frequency values to the first dimensions and lower-frequency values to the latter dimensions.\n",
        "\n",
        "In each sequence, for token i we add to its j-th coordinate the following value:\n",
        "\n",
        "![Positional encoding](./figures/positional_encoding.png \"Positional encoding\").\n",
        "\n",
        "This positional embedding is a function of the number of elements in the sequence and the dimensionality of each element. Thus, it is always a 2-dimensional tensor or “rectangle”.\n",
        "\n",
        "Here is a simple function that, given the number of tokens and the dimensionality of each of them, outputs a matrix where each coordinate (i,j) is the value to be added to token i in dimension j.\n",
        "\n",
        "This positional encoding is added to our model after the linear mapping and the addition of the class token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bOaI_5SrO4vB"
      },
      "outputs": [],
      "source": [
        "def get_positional_embeddings(sequence_length, d):\n",
        "    result = torch.ones(sequence_length, d)\n",
        "    for i in range(sequence_length):\n",
        "        for j in range(d):\n",
        "            result[i][j] = (\n",
        "                np.sin(i / (10000 ** (j / d)))\n",
        "                if j % 2 == 0\n",
        "                else np.cos(i / (10000 ** ((j - 1) / d)))\n",
        "            )\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multi-Head Self-Attention\n",
        "\n",
        "The objective is now that, for a single image, each patch has to be updated based on some similarity measure with the other patches. We do so by linearly mapping each patch (that is now an 8-dimensional vector in our example) to 3 distinct vectors: q, k, and v (query, key, value).\n",
        "\n",
        "Then, for a single patch, we are going to compute the dot product between its q vector with all of the k vectors, divide by the square root of the dimensionality of these vectors (sqrt(8)), softmax these so-called attention cues, and finally multiply each attention cue with the v vectors associated with the different k vectors and sum all up.\n",
        "\n",
        "In this way, each patch assumes a new value that is based on its similarity (after the linear mapping to q, k, and v) with other patches. This whole procedure, however, is carried out H times on H sub-vectors of our current 8-dimensional patches, where H is the number of Heads. \n",
        "\n",
        "Once all results are obtained, they are concatenated together. Finally, the result is passed through a linear layer (for good measure).\n",
        "\n",
        "The intuitive idea behind attention is that it allows modeling the relationship between the inputs. What makes a ‘0’ a zero are not the individual pixel values, but how they relate to each other.\n",
        "\n",
        "This is implemented in the MSA class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "CIoyR-QsOruC"
      },
      "outputs": [],
      "source": [
        "class MSA(nn.Module):\n",
        "    def __init__(self, d, n_heads=2):\n",
        "        super().__init__()\n",
        "        self.d = d\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
        "\n",
        "        d_head = int(d / n_heads)\n",
        "        self.q_mappings = nn.ModuleList(\n",
        "            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
        "        )\n",
        "        self.k_mappings = nn.ModuleList(\n",
        "            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
        "        )\n",
        "        self.v_mappings = nn.ModuleList(\n",
        "            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
        "        )\n",
        "        self.d_head = d_head\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, sequences):\n",
        "        # Sequences has shape (N, seq_length, token_dim)\n",
        "        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
        "        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
        "        result = []\n",
        "        for sequence in sequences:\n",
        "            seq_result = []\n",
        "            for head in range(self.n_heads):\n",
        "                q_mapping = self.q_mappings[head]\n",
        "                k_mapping = self.k_mappings[head]\n",
        "                v_mapping = self.v_mappings[head]\n",
        "\n",
        "                seq = sequence[:, head * self.d_head : (head + 1) * self.d_head]\n",
        "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
        "                # implement attention computation\n",
        "                # for a single patch, we are going to compute the dot product between its q vector with all of the k vectors,\n",
        "                # divide by the square root of the dimensionality of these vectors (sqrt(8)),\n",
        "                # Compute score attention\n",
        "                attention = self.softmax(q @ k.T / (self.d_head**0.5))\n",
        "                seq_result.append(attention @ v)\n",
        "            result.append(torch.hstack(seq_result))\n",
        "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that, for each head, we create distinct Q, K, and V mapping functions (square matrices of size 4x4 in our example).\n",
        "\n",
        "Since our inputs will be sequences of size (N, 50, 8), and we only use 2 heads, we will at some point have an (N, 50, 2, 4) tensor, use a nn.Linear(4, 4) module on it, and then come back, after concatenation, to an (N, 50, 8) tensor.\n",
        "\n",
        "Also notice that using loops is not the most efficient way to compute the multi-head self-attention, but it makes the code much clearer for learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformer Encoder Blocks\n",
        "\n",
        "The next step is to create the transformer encoder block class.\n",
        "\n",
        "Layer normalization (LN) is a popular block that, given an input, subtracts its mean and divides by the standard deviation. It is applied to the last dimension only. We can thus make each of our 50x8 matrices (representing a single sequence) have mean 0 and std 1. After we run our (N, 50, 8) tensor through LN, we still get the same dimensionality.\n",
        "\n",
        "Also, We will be using residual connection that consists in adding the original input to the result of some computation. This, intuitively, allows a network to become more powerful while also preserving the set of possible functions that the model can approximate.\n",
        "\n",
        "We will add a residual connection that will add our original (N, 50, 8) tensor to the (N, 50, 8) obtained after LN and MSA. \n",
        "\n",
        "Next is to add a simple residual connection between what we already have and what we get after passing the current tensor through another LN and an MLP. The MLP is composed of two layers, where the hidden layer typically is four times as big (this is a parameter).\n",
        "\n",
        "The transformer encoder block class (which will be a component of the future ViT class) is thus as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "sv8wnTx4OwP7"
      },
      "outputs": [],
      "source": [
        "class ViTBlock(nn.Module):\n",
        "    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
        "        super().__init__()\n",
        "        self.hidden_d = hidden_d\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(hidden_d)\n",
        "        self.mhsa = MSA(hidden_d, n_heads)\n",
        "        self.norm2 = nn.LayerNorm(hidden_d)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(mlp_ratio * hidden_d, hidden_d),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # implement the forward pass\n",
        "        # First residual connection\n",
        "        out = self.norm1(x)\n",
        "        out = self.mhsa(out)\n",
        "        out = out + x\n",
        "        # Second residual connection\n",
        "        out = self.norm2(out)\n",
        "        out = self.mlp(out)\n",
        "        out = out + x\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ViT model\n",
        "\n",
        "Now that the encoder block is ready, we just need to insert it in our bigger ViT model which is responsible for patchifying before the transformer blocks, and carrying out the classification after.\n",
        "\n",
        "To help classification, we will use an additional **classification token** to the input sequence. This is a special token that we add to our model that has the role of capturing information about the other tokens. This will happen with the MSA block. When information about all other tokens will be present here, we will be able to classify the image using only this special token. The initial value of the special token (the one fed to the transformer encoder) is a parameter of the model that needs to be learned.\n",
        "\n",
        "Thus, we will add a parameter to our model and convert our (N, 49, 8) tokens tensor to an (N, 50, 8) tensor (we add the special token to each sequence).\n",
        "\n",
        "We could have an arbitrary number of transformer blocks. In this example, to keep it simple, I will use only 2. We also add a parameter to know how many heads does each encoder block will use.\n",
        "\n",
        "Finally, we can extract just the classification token (first token) out of our N sequences, and use each token to get N classifications.\n",
        "\n",
        "Since we decided that each token is an 8-dimensional vector, and since we have 10 possible digits, we can implement the classification MLP as a simple 8x10 matrix, activated with the SoftMax function.\n",
        "\n",
        "The output of our model shoud be an (N, 10) tensor. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8Na9BTgnOy3o"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self, chw, n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10):\n",
        "        # Super constructor\n",
        "        super().__init__()\n",
        "\n",
        "        # Attributes\n",
        "        self.chw = chw  # ( C , H , W )\n",
        "        self.n_patches = n_patches\n",
        "        self.n_blocks = n_blocks\n",
        "        self.n_heads = n_heads\n",
        "        self.hidden_d = hidden_d\n",
        "\n",
        "        # Input and patches sizes\n",
        "        assert (\n",
        "            chw[1] % n_patches == 0\n",
        "        ), \"Input shape not entirely divisible by number of patches\"\n",
        "        assert (\n",
        "            chw[2] % n_patches == 0\n",
        "        ), \"Input shape not entirely divisible by number of patches\"\n",
        "        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
        "\n",
        "        # 1) Linear mapper\n",
        "        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
        "        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
        "\n",
        "        # 2) Learnable classification token\n",
        "        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
        "\n",
        "        # 3) Positional embedding\n",
        "        self.register_buffer(\n",
        "            \"positional_embeddings\",\n",
        "            get_positional_embeddings(n_patches**2 + 1, hidden_d),\n",
        "            persistent=False,\n",
        "        )\n",
        "\n",
        "        # 4) Transformer encoder blocks\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [ViTBlock(hidden_d, n_heads) for _ in range(n_blocks)]\n",
        "        )\n",
        "\n",
        "        # 5) Classification MLPk\n",
        "        self.mlp = nn.Sequential(nn.Linear(self.hidden_d, out_d), nn.Softmax(dim=-1))\n",
        "\n",
        "    def forward(self, images):\n",
        "        #\n",
        "        # TO DO: implement the forward pass\n",
        "        #\n",
        "        # Dividing images into patches\n",
        "        n, c, h, w = images.shape\n",
        "        patches = patchify(images, self.n_patches).to(self.positional_embeddings.device)\n",
        "        \n",
        "        # Running linear layer tokenization\n",
        "        # Map the vector corresponding to each patch to the hidden size dimension\n",
        "        tokens = self.linear_mapper(patches)\n",
        "        \n",
        "        # Adding classification token to the tokens\n",
        "        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n",
        "        \n",
        "        # Adding positional embedding\n",
        "        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n",
        "        \n",
        "        # Transformer Blocks\n",
        "        for block in self.blocks:\n",
        "            out = block(out)\n",
        "            \n",
        "        # Getting the classification token only\n",
        "        out = out[:, 0]\n",
        "\n",
        "        # Map to output dimension, output category distribution\n",
        "        out = self.mlp(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ViT training\n",
        "\n",
        "The ViT model being built, the next step is to train it on the MNIST dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we initialize the model and the hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device:  cpu \n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\n",
        "    \"Using device: \",\n",
        "    device,\n",
        "    f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\",\n",
        ")\n",
        "\n",
        "model = ViT(\n",
        "    (1, 28, 28), n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10\n",
        ").to(device)\n",
        "\n",
        "N_EPOCHS = 5\n",
        "LR = 0.005"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training of the ViT model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 loss: 2.07\n",
            "Epoch 2/5 loss: 1.85\n",
            "Epoch 3/5 loss: 1.80\n",
            "Epoch 4/5 loss: 1.77\n",
            "Epoch 5/5 loss: 1.74\n"
          ]
        }
      ],
      "source": [
        "optimizer = Adam(model.parameters(), lr=LR)\n",
        "criterion = CrossEntropyLoss()\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        x, y = batch\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_hat = model(x)\n",
        "        loss = criterion(y_hat, y)\n",
        "\n",
        "        train_loss += loss.detach().cpu().item() / len(train_loader)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ViT test\n",
        "\n",
        "Finally, let's test the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "h55dVGGhOaPI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 1.71\n",
            "Test accuracy: 75.41%\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    correct, total = 0, 0\n",
        "    test_loss = 0.0\n",
        "    for batch in test_loader:\n",
        "        x, y = batch\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_hat = model(x)\n",
        "        loss = criterion(y_hat, y)\n",
        "        test_loss += loss.detach().cpu().item() / len(test_loader)\n",
        "        correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
        "        total += len(x)\n",
        "    print(f\"Test loss: {test_loss:.2f}\")\n",
        "    print(f\"Test accuracy: {correct / total * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Further experiments\n",
        "\n",
        "1. Adapt the code to apply the ViT model on CIFAR dataset.\n",
        "2. Make use of a validation set to evaluate overfitting.\n",
        "3. Evaluate the model with a dimension of 16 for the tokens and 4 encoder blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import CIFAR dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "let's import the CIFAR dataset but without validation split first "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from torchvision.datasets import CIFAR10\n",
        "import numpy as np \n",
        "\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "     \n",
        "# Loading data\n",
        "transform = ToTensor()\n",
        "\n",
        "train_set = CIFAR10(\n",
        "        root=\"./../datasets\", train=True, download=True, transform=transform\n",
        ")\n",
        "test_set = CIFAR10(\n",
        "        root=\"./../datasets\", train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_set, shuffle=True, batch_size=128)\n",
        "test_loader = DataLoader(test_set, shuffle=False, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multi-Head Self-Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nothing to change from the previous implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyMSA(nn.Module):\n",
        "    def __init__(self, d, n_heads=2):\n",
        "        super(MyMSA, self).__init__()\n",
        "        self.d = d\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
        "\n",
        "        d_head = int(d / n_heads)\n",
        "        self.q_mappings = nn.ModuleList(\n",
        "            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
        "        )\n",
        "        self.k_mappings = nn.ModuleList(\n",
        "            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
        "        )\n",
        "        self.v_mappings = nn.ModuleList(\n",
        "            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
        "        )\n",
        "        self.d_head = d_head\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, sequences):\n",
        "        # Sequences has shape (N, seq_length, token_dim)\n",
        "        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
        "        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
        "        result = []\n",
        "        for sequence in sequences:\n",
        "            seq_result = []\n",
        "            for head in range(self.n_heads):\n",
        "                q_mapping = self.q_mappings[head]\n",
        "                k_mapping = self.k_mappings[head]\n",
        "                v_mapping = self.v_mappings[head]\n",
        "\n",
        "                seq = sequence[:, head * self.d_head : (head + 1) * self.d_head]\n",
        "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
        "\n",
        "                attention = self.softmax(q @ k.T / (self.d_head**0.5))\n",
        "                seq_result.append(attention @ v)\n",
        "            result.append(torch.hstack(seq_result))\n",
        "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformer Encoder Blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nothing to change from the previous implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyViTBlock(nn.Module):\n",
        "    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
        "        super(MyViTBlock, self).__init__()\n",
        "        self.hidden_d = hidden_d\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(hidden_d)\n",
        "        self.mhsa = MyMSA(hidden_d, n_heads)\n",
        "        self.norm2 = nn.LayerNorm(hidden_d)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(mlp_ratio * hidden_d, hidden_d),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.mhsa(self.norm1(x))\n",
        "        out = out + self.mlp(self.norm2(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ViT model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The number of output classes (out_d) is set to 10, which corresponds to the number of classes in CIFAR-10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyViT(nn.Module):\n",
        "    def __init__(self, chw, n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10):\n",
        "        # Super constructor\n",
        "        super(MyViT, self).__init__()\n",
        "\n",
        "        # Attributes\n",
        "        self.chw = chw  # ( C , H , W )\n",
        "        self.n_patches = n_patches\n",
        "        self.n_blocks = n_blocks\n",
        "        self.n_heads = n_heads\n",
        "        self.hidden_d = hidden_d\n",
        "\n",
        "        # Input and patches sizes\n",
        "        assert (\n",
        "            chw[1] % n_patches == 0\n",
        "        ), \"Input shape not entirely divisible by the number of patches\"\n",
        "        assert (\n",
        "            chw[2] % n_patches == 0\n",
        "        ), \"Input shape not entirely divisible by the number of patches\"\n",
        "        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
        "\n",
        "        # 1) Linear mapper\n",
        "        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
        "        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
        "\n",
        "        # 2) Learnable classification token\n",
        "        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
        "\n",
        "        # 3) Positional embedding\n",
        "        self.register_buffer(\n",
        "            \"positional_embeddings\",\n",
        "            get_positional_embeddings(n_patches**2 + 1, hidden_d),\n",
        "            persistent=False,\n",
        "        )\n",
        "\n",
        "        # 4) Transformer encoder blocks\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)]\n",
        "        )\n",
        "\n",
        "        # 5) Classification MLP\n",
        "        self.mlp = nn.Sequential(nn.Linear(self.hidden_d, out_d), nn.Softmax(dim=-1))\n",
        "\n",
        "    def forward(self, images):\n",
        "        # Dividing images into patches\n",
        "        n, c, h, w = images.shape\n",
        "        patches = patchify(images, self.n_patches).to(self.positional_embeddings.device)\n",
        "\n",
        "        # Running linear layer tokenization\n",
        "        # Map the vector corresponding to each patch to the hidden size dimension\n",
        "        tokens = self.linear_mapper(patches)\n",
        "\n",
        "        # Adding classification token to the tokens\n",
        "        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n",
        "\n",
        "        # Adding positional embedding\n",
        "        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n",
        "\n",
        "        # Transformer Blocks\n",
        "        for block in self.blocks:\n",
        "            out = block(out)\n",
        "\n",
        "        # Getting the classification token only\n",
        "        out = out[:, 0]\n",
        "\n",
        "        return self.mlp(out)  # Map to output dimension, output category distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ViT training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Updated the input dimensions of the model to (3, 32, 32) of the CIFAR images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device:  cpu \n"
          ]
        }
      ],
      "source": [
        "# Defining model and training options\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\n",
        "        \"Using device: \",\n",
        "        device,\n",
        "        f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\",\n",
        "    )\n",
        "model = MyViT(\n",
        "    (3, 32, 32), n_patches=8, n_blocks=2, hidden_d=8, n_heads=2, out_d=10\n",
        ").to(device)\n",
        "N_EPOCHS = 5\n",
        "LR = 0.005\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training loop (without validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 loss: 2.21\n",
            "Epoch 2/5 loss: 2.15\n",
            "Epoch 3/5 loss: 2.13\n",
            "Epoch 4/5 loss: 2.12\n",
            "Epoch 5/5 loss: 2.11\n"
          ]
        }
      ],
      "source": [
        "optimizer = Adam(model.parameters(), lr=LR)\n",
        "criterion = CrossEntropyLoss()\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        x, y = batch\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_hat = model(x)\n",
        "        loss = criterion(y_hat, y)\n",
        "\n",
        "        train_loss += loss.detach().cpu().item() / len(train_loader)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 2.11\n",
            "Test accuracy: 33.91%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "with torch.no_grad():\n",
        "    correct, total = 0, 0\n",
        "    test_loss = 0.0\n",
        "    for batch in test_loader:\n",
        "        x, y = batch\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_hat = model(x)\n",
        "        loss = criterion(y_hat, y)\n",
        "        test_loss += loss.detach().cpu().item() / len(test_loader)\n",
        "\n",
        "        correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
        "        total += len(x)\n",
        "    print(f\"Test loss: {test_loss:.2f}\")\n",
        "    print(f\"Test accuracy: {correct / total * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From 75.41% (MNIMST dataset) to 33.91% (CIFAR dataset) in the test accuracy ! \n",
        "Let's a hope better preformance with validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading data and creating a validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "transform = ToTensor()\n",
        "\n",
        "dataset = CIFAR10(root=\"./datasets\", train=True, download=True, transform=transform)\n",
        "\n",
        "# Splitting dataset into train and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_set, shuffle=True, batch_size=128)\n",
        "val_loader = DataLoader(val_set, shuffle=False, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use a model with 16 tokens and 4 encoder blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device:  cpu \n"
          ]
        }
      ],
      "source": [
        "# Defining model and training options\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\n",
        "        \"Using device: \",\n",
        "        device,\n",
        "        f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\",\n",
        "    )\n",
        "    \n",
        "# Use a model with 16 tokens and 4 encoder blocks\n",
        "model = MyViT(\n",
        "        (3, 32, 32), n_patches=8, n_blocks=4, hidden_d=16, n_heads=2, out_d=10\n",
        "    ).to(device)\n",
        "    \n",
        "N_EPOCHS = 5\n",
        "LR = 0.005"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 Train Loss: 2.21\n",
            "Epoch 1/5 Validation Loss: 2.16\n",
            "Epoch 1/5 Validation Accuracy: 29.06%\n",
            "Epoch 2/5 Train Loss: 2.14\n",
            "Epoch 2/5 Validation Loss: 2.12\n",
            "Epoch 2/5 Validation Accuracy: 32.87%\n",
            "Epoch 3/5 Train Loss: 2.11\n",
            "Epoch 3/5 Validation Loss: 2.11\n",
            "Epoch 3/5 Validation Accuracy: 34.11%\n",
            "Epoch 4/5 Train Loss: 2.10\n",
            "Epoch 4/5 Validation Loss: 2.10\n",
            "Epoch 4/5 Validation Accuracy: 35.16%\n",
            "Epoch 5/5 Train Loss: 2.09\n",
            "Epoch 5/5 Validation Loss: 2.09\n",
            "Epoch 5/5 Validation Accuracy: 36.61%\n"
          ]
        }
      ],
      "source": [
        "optimizer = Adam(model.parameters(), lr=LR)\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "list_train_loss = []\n",
        "list_valid_loss = []\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        x, y = batch\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_hat = model(x)\n",
        "        loss = criterion(y_hat, y)\n",
        "\n",
        "        train_loss += loss.detach().cpu().item() / len(train_loader)\n",
        "        list_train_loss.append(train_loss)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{N_EPOCHS} Train Loss: {train_loss:.2f}\")\n",
        "\n",
        "    # Validation loop\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "        for batch in val_loader:\n",
        "            x_val, y_val = batch\n",
        "            x_val, y_val = x_val.to(device), y_val.to(device)\n",
        "            y_hat_val = model(x_val)\n",
        "            loss_val = criterion(y_hat_val, y_val)\n",
        "            val_loss += loss_val.detach().cpu().item() / len(val_loader)\n",
        "\n",
        "            list_valid_loss.append(val_loss)\n",
        "\n",
        "            correct_val += torch.sum(\n",
        "                    torch.argmax(y_hat_val, dim=1) == y_val\n",
        "                ).detach().cpu().item()\n",
        "            total_val += len(x_val)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{N_EPOCHS} Validation Loss: {val_loss:.2f}\")\n",
        "        print(f\"Epoch {epoch + 1}/{N_EPOCHS} Validation Accuracy: {correct_val / total_val * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "list_train_loss = [2.21, 2.14, 2.11, 2.10, 2.09]\n",
        "list_valid_loss = [2.16, 2.12, 2.11, 2.10, 2.09]\n",
        "plt.plot(range(len(list_train_loss)), list_train_loss, label='Train Loss')\n",
        "plt.plot(range(len(list_valid_loss)), list_valid_loss, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loss: Both the training and validation loss values are decreasing over time, which is a good sign that the model is learning and improving its predictions over each epoch.\n",
        "\n",
        "Training Loss: This is the loss calculated from the training dataset, which the model directly learns from. It starts higher and consistently decreases, which suggests that the model is fitting the training data well.\n",
        "\n",
        "Validation Loss: Represented by the orange line, the loss calculated from a set of data not seen by the model during training (validation dataset). It's important for this loss to decrease and for the validation curve to follow the training curve closely. If the validation loss is much higher than the training loss, it could indicate overfitting. However, in this case, the validation loss is also decreasing and is quite close to the training loss, which suggests the model is generalizing well to new data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "let's test ou model trained in our test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Test Loss: 2.08\n",
            "Test Accuracy: 37.59%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "test_set = CIFAR10(\n",
        "        root=\"./datasets\", train=False, download=True, transform=transform\n",
        "    )\n",
        "test_loader = DataLoader(test_set, shuffle=False, batch_size=128)\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct_test, total_test = 0, 0\n",
        "    test_loss = 0.0\n",
        "    for batch in test_loader:\n",
        "        x_test, y_test = batch\n",
        "        x_test, y_test = x_test.to(device), y_test.to(device)\n",
        "        y_hat_test = model(x_test)\n",
        "        loss_test = criterion(y_hat_test, y_test)\n",
        "        test_loss += loss_test.detach().cpu().item() / len(test_loader)\n",
        "\n",
        "        correct_test += torch.sum(torch.argmax(y_hat_test, dim=1) == y_test).detach().cpu().item()\n",
        "        total_test += len(x_test)\n",
        "\n",
        "    print(f\"Test Loss: {test_loss:.2f}\")\n",
        "    print(f\"Test Accuracy: {correct_test / total_test * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From 75.41% (MNIMST dataset) to 33.91% (CIFAR dataset without validation) to 37.59% (CIFAR dataset with validation) in the test accuracy ! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The drop in test accuracy from MNIST to CIFAR-10 is expected because CIFAR-10 is a more challenging dataset with color images and a larger number of classes. The accuracy drop can be attributed to the increased complexity of the images and the need for the model to learn more intricate features.\n",
        "\n",
        "The addition of a validation set allows us to monitor overfitting during training. In our case, the test accuracy slightly increased when using a validation set, which is a positive sign (from overfitting). However, the overall test accuracy on CIFAR-10 is lower than that on MNIST, which is consistent with the dataset's increased complexity. Maybe with hyperparameter tuning, data augmentation we could do better !"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
